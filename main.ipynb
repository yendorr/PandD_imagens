{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = 'images'\n",
    "image_files = sorted([f for f in os.listdir(images_folder) if f.endswith('.jpg') or f.endswith('.png')], key=lambda x: int(os.path.splitext(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para codificar a imagem em base64\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média da largura: 835.3571428571429\n",
      "Média da altura: 1514.357142857143\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    with Image.open(os.path.join(images_folder, image_file)) as img:\n",
    "        widths.append(img.width)\n",
    "        heights.append(img.height)\n",
    "\n",
    "mean_width = np.mean(widths)\n",
    "mean_height = np.mean(heights)\n",
    "\n",
    "print(f'Média da largura: {mean_width}')\n",
    "print(f'Média da altura: {mean_height}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "def upload_to_gemini(path, mime_type=None):\n",
    "  \"\"\"Uploads the given file to Gemini.\n",
    "\n",
    "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
    "  \"\"\"\n",
    "  file = genai.upload_file(path, mime_type=mime_type)\n",
    "  return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:33<00:00,  3.33s/it]\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "  upload_to_gemini(f\"{images_folder}/{image}\", mime_type=\"image/jpeg\") for image in tqdm(image_files)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:38<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemini-2.0-flash-thinking-exp-1219\"\n",
    "logs_folder = \"logs/gemini\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            tipo_imagem, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=model_name,\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "    file_index = int(os.path.splitext(image)[0])\n",
    "    chat_session = model.start_chat(\n",
    "    history=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "            files[file_index],\n",
    "        ],\n",
    "        },\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    response = chat_session.send_message(prompt)\n",
    "    resultado = response.text\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 4 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:15<00:00,  6.98s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "logs_folder = \"logs/gpt4_mini\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            tipo_imagem, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "    # Path to your image\n",
    "    image_path = f\"{images_folder}/{image}\"\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    \n",
    "    resultado = response.choices[0].message.content\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [02:16<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "logs_folder = \"logs/gpt4\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            tipo_imagem, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "    # Path to your image\n",
    "    image_path = f\"{images_folder}/{image}\"\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    \n",
    "    resultado = response.choices[0].message.content\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groq Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:31<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3.2-11b-vision-preview\"\n",
    "logs_folder = \"logs/groq_mini\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            image_type, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "\n",
    "    image_path = f\"{images_folder}/{image}\"\n",
    "\n",
    "    # Codificar a imagem em base64\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "    # Criar a solicitação de completions\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    resultado = chat_completion.choices[0].message.content\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groq Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:29<00:00,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama-3.2-90b-vision-preview\"\n",
    "logs_folder = \"logs/groq\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            image_type, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "\n",
    "    image_path = f\"{images_folder}/{image}\"\n",
    "\n",
    "    # Codificar a imagem em base64\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "    # Criar a solicitação de completions\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    resultado = chat_completion.choices[0].message.content\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 717.86it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"claude-3-5-sonnet-20241022\"\n",
    "logs_folder = \"logs/claude\"\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            tipo_imagem, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "\n",
    "    # Path to your image\n",
    "    image_path = f\"{images_folder}/{image}\"\n",
    "\n",
    "    image_data = encode_image_to_base64(image_path)\n",
    "    media_type = \"image/jpeg\" \n",
    "\n",
    "\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=1024*8,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": media_type,\n",
    "                            \"data\": image_data,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    resultado = message.content[0].text\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:   f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo https://build.nvidia.com/nvidia/neva-22b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO https://build.nvidia.com/microsoft/phi-3-vision-128k-instruct?snippet_tab=Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "api_key = os.getenv(\"PHI3V_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "def upload_to_nvidia(file_path):\n",
    "    jwt_token = os.getenv(\"PHI3V_API_KEY\")\n",
    "\n",
    "    # Define o URL da API e o JWT Token de autenticação\n",
    "    url = 'https://api.nvcf.nvidia.com/v2/nvcf/assets'\n",
    "\n",
    "\n",
    "    # Cabeçalhos da requisição\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {jwt_token}',\n",
    "        'Accept': 'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    # Corpo da requisição\n",
    "    data = {\n",
    "        \"contentType\": \"image/png\",\n",
    "        \"description\": \"MQ_chat_image\"\n",
    "    }\n",
    "\n",
    "    # Realiza a requisição POST\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    # Verifica se a requisição foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        # Extrai o assetId e a uploadUrl do corpo da resposta\n",
    "        response_data = response.json()\n",
    "        asset_id = response_data.get(\"assetId\")\n",
    "        upload_url = response_data.get(\"uploadUrl\")\n",
    "        \n",
    "        pass\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        pass\n",
    "\n",
    "    # Cabeçalhos da requisição PUT\n",
    "    headers_put = {\n",
    "        'Content-Type': 'image/png',\n",
    "        'x-amz-meta-nvcf-asset-description': 'MQ_chat_image'\n",
    "    }\n",
    "\n",
    "    # Realiza a requisição PUT para o upload do arquivo\n",
    "    with open(file_path, 'rb') as file:\n",
    "        response_put = requests.put(upload_url, headers=headers_put, data=file)\n",
    "\n",
    "    # Verifica se o upload foi bem-sucedido\n",
    "    if response_put.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        pass\n",
    "    print(response.json()['uploadUrl'])\n",
    "    return response.json()['uploadUrl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://nv-gdn-strap-assets-prd.s3-accelerate.amazonaws.com/0ibafc5Ew8_-nYmbYjgVKav656KCJNKbuEmqzxLAJXo/1dedf37a-99b3-4d43-8353-27151e87aa66?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQC5cDBGYz28%2FFbwC0JVrVukEkxqP6dPdS5ExDmpcC4sfQIgA7%2FbS%2BR4L1fLiyYivZZg2vHtFwdj%2B1%2F2VYiJWVLmSV4qmgUIfBACGgwwNTIyNzc1MjgxMjIiDFGSxtcDjFANnXxXTir3BGpeT2FnfUkeV9Xfn7PJhrV1XCKWpGGYZnshmLzYAFezn5Jwi3kZ8TiR4XdfhGXdHezIElLt0CbV2BUBuHfThS6rIAJKfuXsu2cRO5vdH10C9EKTHd1ZRLHEYPeOMKl3DBiB6YxEJEq062in22C4SeslIuJHUPff8qRvAvcF15WHphqyzrDySu%2BZB3QRQz2pwHmDdKX1FUHeNGa%2Bgcu%2B1m0HbMcL0edDHbhumht4zvcxOZsU%2B6pHUsM6eRaJ9wrCx4BMR5a3WhROWoTu6oK9yqCP8KBjCkjHf6eFElcdvgNvoypvziqMMa3PzeHxizAn3A6MCsbycJA%2BoeDl7V%2FTUxk1%2B7V7ItWs2Vqtsy1qtQ77qb%2BRmZkf1kIelxFCh0vFEN1Lt49SKQoF2o5TdAUtXWbqN8WS5HXOf0lr%2FPi%2BzlmU%2BS%2FBmayfSC9mF4NAQEfqSZVBlMpaYpPw2eI8axChs9Bf5qwIPubkDvTTBz9x2rvaZJo68jGbX%2BOdo%2BCvJXZ1vuEcN66ODkAjj6J4Gy8eoJ15b1njpIvcfnoA8gufU%2FVb58mVTpHZVG%2BEPXtYNSRWOmqeP9T0SxjYT%2F5ikcbUjaAwIzfx9Rt5E%2BxXGCgYfTzosPN%2BUIGr63byzvKt1HgNVoBZ9h2bnfNSxVWzN7v0DTkxQj2mjMxsWeiIGWMu77%2FCV4SYUoMlTOd1KLob54ZBlQeIQmqz0lP3BBFg9tniEJCDlubL17YsCaDXCyx2tlcd9VOqe%2B0wX6JbWwWr9UsgNkIbKt33rqF3ibf3SH8uFWCbluq0gSM5WR1Q%2B6oNkGqfjCsBkCS%2FIqDTxhIt%2BFKgXHDAJQ40R%2BAwiZL7uwY6mwGO49tEqDyGIYnykoHLV0Rfg5oTleXxi8uZcalu3eBAjecS1GzEZWYQu9m%2FSLgyc0z%2Fpu02alZFogE%2FmAvoPD8FsEE9LmnC8LmWP9BrjI53xMYBRUJk9%2FW1XZkjxlb6zVJMdVdW3Noyv21dpGbYGHUMf4PC2aopI6c%2FmV%2FBDqfAJ6wVmP026LPRGlVYuyuyd0ccCT3EfB7UOOdhoA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250108T185813Z&X-Amz-SignedHeaders=content-type%3Bhost%3Bx-amz-meta-nvcf-asset-description&X-Amz-Credential=ASIAQYK73YY5BOYGDJQP%2F20250108%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Expires=2100&X-Amz-Signature=968fcb509c02b578a7ff12e0fdacf39e51d342a39f01253c27e097c2326bbb39\n",
      "https://nv-gdn-strap-assets-prd.s3-accelerate.amazonaws.com/0ibafc5Ew8_-nYmbYjgVKav656KCJNKbuEmqzxLAJXo/1dedf37a-99b3-4d43-8353-27151e87aa66?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQC5cDBGYz28%2FFbwC0JVrVukEkxqP6dPdS5ExDmpcC4sfQIgA7%2FbS%2BR4L1fLiyYivZZg2vHtFwdj%2B1%2F2VYiJWVLmSV4qmgUIfBACGgwwNTIyNzc1MjgxMjIiDFGSxtcDjFANnXxXTir3BGpeT2FnfUkeV9Xfn7PJhrV1XCKWpGGYZnshmLzYAFezn5Jwi3kZ8TiR4XdfhGXdHezIElLt0CbV2BUBuHfThS6rIAJKfuXsu2cRO5vdH10C9EKTHd1ZRLHEYPeOMKl3DBiB6YxEJEq062in22C4SeslIuJHUPff8qRvAvcF15WHphqyzrDySu%2BZB3QRQz2pwHmDdKX1FUHeNGa%2Bgcu%2B1m0HbMcL0edDHbhumht4zvcxOZsU%2B6pHUsM6eRaJ9wrCx4BMR5a3WhROWoTu6oK9yqCP8KBjCkjHf6eFElcdvgNvoypvziqMMa3PzeHxizAn3A6MCsbycJA%2BoeDl7V%2FTUxk1%2B7V7ItWs2Vqtsy1qtQ77qb%2BRmZkf1kIelxFCh0vFEN1Lt49SKQoF2o5TdAUtXWbqN8WS5HXOf0lr%2FPi%2BzlmU%2BS%2FBmayfSC9mF4NAQEfqSZVBlMpaYpPw2eI8axChs9Bf5qwIPubkDvTTBz9x2rvaZJo68jGbX%2BOdo%2BCvJXZ1vuEcN66ODkAjj6J4Gy8eoJ15b1njpIvcfnoA8gufU%2FVb58mVTpHZVG%2BEPXtYNSRWOmqeP9T0SxjYT%2F5ikcbUjaAwIzfx9Rt5E%2BxXGCgYfTzosPN%2BUIGr63byzvKt1HgNVoBZ9h2bnfNSxVWzN7v0DTkxQj2mjMxsWeiIGWMu77%2FCV4SYUoMlTOd1KLob54ZBlQeIQmqz0lP3BBFg9tniEJCDlubL17YsCaDXCyx2tlcd9VOqe%2B0wX6JbWwWr9UsgNkIbKt33rqF3ibf3SH8uFWCbluq0gSM5WR1Q%2B6oNkGqfjCsBkCS%2FIqDTxhIt%2BFKgXHDAJQ40R%2BAwiZL7uwY6mwGO49tEqDyGIYnykoHLV0Rfg5oTleXxi8uZcalu3eBAjecS1GzEZWYQu9m%2FSLgyc0z%2Fpu02alZFogE%2FmAvoPD8FsEE9LmnC8LmWP9BrjI53xMYBRUJk9%2FW1XZkjxlb6zVJMdVdW3Noyv21dpGbYGHUMf4PC2aopI6c%2FmV%2FBDqfAJ6wVmP026LPRGlVYuyuyd0ccCT3EfB7UOOdhoA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250108T185813Z&X-Amz-SignedHeaders=content-type%3Bhost%3Bx-amz-meta-nvcf-asset-description&X-Amz-Credential=ASIAQYK73YY5BOYGDJQP%2F20250108%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Expires=2100&X-Amz-Signature=968fcb509c02b578a7ff12e0fdacf39e51d342a39f01253c27e097c2326bbb39\n",
      "https://nv-gdn-strap-assets-prd.s3-accelerate.amazonaws.com/0ibafc5Ew8_-nYmbYjgVKav656KCJNKbuEmqzxLAJXo/1dedf37a-99b3-4d43-8353-27151e87aa66?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQC5cDBGYz28%2FFbwC0JVrVukEkxqP6dPdS5ExDmpcC4sfQIgA7%2FbS%2BR4L1fLiyYivZZg2vHtFwdj%2B1%2F2VYiJWVLmSV4qmgUIfBACGgwwNTIyNzc1MjgxMjIiDFGSxtcDjFANnXxXTir3BGpeT2FnfUkeV9Xfn7PJhrV1XCKWpGGYZnshmLzYAFezn5Jwi3kZ8TiR4XdfhGXdHezIElLt0CbV2BUBuHfThS6rIAJKfuXsu2cRO5vdH10C9EKTHd1ZRLHEYPeOMKl3DBiB6YxEJEq062in22C4SeslIuJHUPff8qRvAvcF15WHphqyzrDySu%2BZB3QRQz2pwHmDdKX1FUHeNGa%2Bgcu%2B1m0HbMcL0edDHbhumht4zvcxOZsU%2B6pHUsM6eRaJ9wrCx4BMR5a3WhROWoTu6oK9yqCP8KBjCkjHf6eFElcdvgNvoypvziqMMa3PzeHxizAn3A6MCsbycJA%2BoeDl7V%2FTUxk1%2B7V7ItWs2Vqtsy1qtQ77qb%2BRmZkf1kIelxFCh0vFEN1Lt49SKQoF2o5TdAUtXWbqN8WS5HXOf0lr%2FPi%2BzlmU%2BS%2FBmayfSC9mF4NAQEfqSZVBlMpaYpPw2eI8axChs9Bf5qwIPubkDvTTBz9x2rvaZJo68jGbX%2BOdo%2BCvJXZ1vuEcN66ODkAjj6J4Gy8eoJ15b1njpIvcfnoA8gufU%2FVb58mVTpHZVG%2BEPXtYNSRWOmqeP9T0SxjYT%2F5ikcbUjaAwIzfx9Rt5E%2BxXGCgYfTzosPN%2BUIGr63byzvKt1HgNVoBZ9h2bnfNSxVWzN7v0DTkxQj2mjMxsWeiIGWMu77%2FCV4SYUoMlTOd1KLob54ZBlQeIQmqz0lP3BBFg9tniEJCDlubL17YsCaDXCyx2tlcd9VOqe%2B0wX6JbWwWr9UsgNkIbKt33rqF3ibf3SH8uFWCbluq0gSM5WR1Q%2B6oNkGqfjCsBkCS%2FIqDTxhIt%2BFKgXHDAJQ40R%2BAwiZL7uwY6mwGO49tEqDyGIYnykoHLV0Rfg5oTleXxi8uZcalu3eBAjecS1GzEZWYQu9m%2FSLgyc0z%2Fpu02alZFogE%2FmAvoPD8FsEE9LmnC8LmWP9BrjI53xMYBRUJk9%2FW1XZkjxlb6zVJMdVdW3Noyv21dpGbYGHUMf4PC2aopI6c%2FmV%2FBDqfAJ6wVmP026LPRGlVYuyuyd0ccCT3EfB7UOOdhoA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250108T185813Z&X-Amz-SignedHeaders=content-type%3Bhost%3Bx-amz-meta-nvcf-asset-description&X-Amz-Credential=ASIAQYK73YY5BOYGDJQP%2F20250108%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Expires=2100&X-Amz-Signature=968fcb509c02b578a7ff12e0fdacf39e51d342a39f01253c27e097c2326bbb39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 3/28 [00:49<06:52, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "invoke_url = \"https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct\"\n",
    "stream = False\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
    "}\n",
    "\n",
    "model_name = \"phi-3-vision-128k-instruct\"\n",
    "logs_folder = \"logs/phi3v\"\n",
    "\n",
    "prompt = \"\"\"Extraia as informações dessa imagem, \n",
    "            Responda apenas com um json, com os campos:\n",
    "            is_doc, que é um booleano que indica se o foco da imagem é um documento,\n",
    "            tipo_imagem, que é uma string que indica o tipo da imagem,\n",
    "            info, que é um dicionário com as informações estruturadas extraídas da imagem.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Crie a pasta de logs, se não existir\n",
    "os.makedirs(logs_folder, exist_ok=True)\n",
    "\n",
    "for image in tqdm(image_files):\n",
    "    # Criar o nome do arquivo de log com base na URL\n",
    "    log_file = os.path.join(logs_folder, os.path.splitext(image)[0] + '.log')\n",
    "\n",
    "    # Pular se já processado\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as f:\n",
    "            if f.read().strip().endswith('EOF'):\n",
    "                continue\n",
    "    \n",
    "    image_url = upload_to_nvidia(f\"{images_folder}/{image}\")\n",
    "    print(image_url)\n",
    "    # Criar o payload com a URL da imagem\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'o que há na imagem? <img src=\"{image_url}\" />'\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.01,\n",
    "        \"top_p\": 0.70,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "\n",
    "    # Fazer a requisição POST\n",
    "    response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "    \n",
    "    # Verificar resposta da API\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erro ao processar {image}: {response.status_code}\")\n",
    "        print(response.json())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        resultado = response.json()['choices'][0]['message']['content']\n",
    "    except KeyError:\n",
    "        print(f\"Erro no formato da resposta para {image}\")\n",
    "        print(response.json())\n",
    "        continue\n",
    "\n",
    "    # Escrever resultado no arquivo de log\n",
    "    with open(log_file, 'a') as f:\n",
    "        if os.path.getsize(log_file) > 0:\n",
    "            f.write('\\n' * 5)\n",
    "        f.write(f'model_name\\n{model_name}\\n\\nprompt\\n{prompt}\\n\\nresultado\\n{resultado}\\n\\nEOF')\n",
    "    break\n",
    "print(\"Processamento concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 77 ativos:\n",
      "ID: 03b01463-6636-4a3e-9e65-59fda492f746, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 040791a5-792d-4797-9d4c-27482d1d133d, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 06950a09-a84d-46a2-b4ba-f548f2871b4f, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 069fe330-3030-4dce-8c25-2a46fc870a29, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 0912eec2-95a0-481f-98ef-f1c530abfa51, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 09f9d4d5-667b-4572-adc4-ba4d2d1fb99b, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 0a538604-c5ef-4bde-92dc-7af8b458973a, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 11c8928a-1e91-4410-b50e-fd2dfaa09a7b, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 131b4aff-904c-4f82-85b7-065c76055768, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 13ec4c9d-b6dd-4d81-8f24-48cb5c7e63aa, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 148d4021-e36f-4984-88e7-1d966b16e0da, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 15d553c0-c3ce-439a-94be-f93f557621cd, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 1dedf37a-99b3-4d43-8353-27151e87aa66, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 1e29fa78-6d75-441a-8170-cf27e86d77d2, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 1fa2be5f-c30f-4e76-ae54-d731756a96cd, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 21026bab-b5d9-4c9e-b835-1dd6362cbd43, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 22cd5b25-d109-445a-b84f-47d1b3422fe8, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 2dbab7ea-d1f9-4324-8105-793b41ea6fce, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 2e484a55-2f5e-4a3d-9dad-69671dc538e1, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 2e9f7e01-b307-4c24-8e00-23c62428a5ca, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 3131f1e7-6f56-48b8-b803-0c03828db8f9, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 328b38b7-4765-4c5d-acd0-7b22f8ca5146, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 39927e53-26a0-4a8e-816e-d9a5bf9552cf, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 3aca26be-7dc7-408e-95fb-c4ff1bee3fd2, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 3bbc6fb6-dd10-42c3-bd7d-5c5643e7d339, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 457d111a-d019-425c-84a4-ace653999569, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 4679eed5-240a-4bed-9e1d-e3c77a712a08, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 4d6b61cb-f03d-40b8-afc8-2779599de4d9, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 55d7843e-e881-4980-ad53-9fb99ad76f8f, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 5a4981ac-2f64-43c1-9975-c3ce13ef439b, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 5cbc9e84-3c7b-4dad-a6b8-6fd7ab5a4e99, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 60901659-1ce6-4d52-9884-bf0af354f918, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 60adebbc-131c-4d51-ab6e-bd7a3d938ac1, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 64a99f14-74ce-4148-a840-8126f30d26a7, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 662d36a4-557b-4eac-a51d-afcfa583a660, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 673beb24-281f-4ca8-9df6-68ba35647473, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 69291fa3-56e1-4ef8-9a7e-71ba66c1438a, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 70f7d137-6747-4d7d-8a9f-f9bd7911eaf2, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 722c2fa1-baac-42d2-9416-051fdf06f493, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 72e6f0ba-01da-486d-87c9-9192586542a0, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 77dea9ba-c4b2-4f1f-aba5-5fb864b1103a, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 816aa813-0329-4b92-bdc9-45e30a9c3904, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 8565aa40-a6b8-4922-b658-7c1b065efd1e, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 8a1a2d6c-ea0c-4aae-98b8-14b92f763819, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 8c0e5226-a923-4632-b2ca-e90b533eac50, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 934c786c-1c3c-49d8-b81c-696165fa8fdb, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 93736b16-8e7c-4bb2-95f3-a8d70fc4c77c, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 94ca562e-a2c9-4ba1-90fa-23779b2e0993, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 95761616-bfa3-43e5-948b-d7753305a7d8, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 982498ec-0eac-40f1-99a5-500eb26581fe, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 99d10fbf-f642-44c5-aaaf-a8ce62099097, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 9b16c31e-cdbd-4e83-80ce-cbba4c326793, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: 9c8b4a30-9148-436e-afe3-66174939d8d7, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: a5c44e78-ac68-4f7a-b5a3-8f0b60fedc07, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: a77c3be5-bc7e-48ac-9db5-4fcf7f3c65bc, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: a95af678-cf46-4f39-b341-6a0885621b5b, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: b2e88a3c-9fa3-4e92-9534-e083e37b55fb, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: b6d83bea-5b63-4821-9d1a-8545398ec741, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: b794bd0f-2e54-4dc7-b79d-2830db1873ad, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: b93ad458-3201-4b3d-9324-79a52dde4143, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: bb89b32a-3134-43cf-9b1b-d6bcb4f3c992, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: bd2e07d9-cafa-408b-a607-e7c404f92d7c, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: c1e4f8be-9957-47f6-87dd-3f975ecabbf2, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: c430dc91-e9a4-4c9f-8dbd-6a822312e860, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: c466d453-9d49-4cc0-b97e-baaf273fcc45, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: c7e0fe77-7049-4a8c-9a34-91d450404e91, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: cb1a6c64-113f-40d0-9cc5-275ae5447411, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: cbbbcbb6-43af-41c5-ae44-097c5b9b610c, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: ce908658-059c-46f3-8842-0f0472fc1fb9, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: d1432645-f935-4f2c-b0bf-f00331dba2ce, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: d2458813-bd79-4fe3-8cd3-695c60f58aea, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: d6882c4b-9ca5-4eb6-82fc-4737249c3545, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: d8fca2d2-1758-4384-9265-6ab4d6f7f93c, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: e29ec183-e0e8-4d36-a50d-eda1aca5b481, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: e3406df9-7dd7-4f0d-83ef-a1d9b0124dc4, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: e73b6665-c823-4392-a003-3e6c32528aa3, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n",
      "ID: eb7f3024-9a5a-4876-ad65-52f3e23ca7e2, URL: None, Descrição: None\n",
      "dict_keys(['assetId'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL da API para listar os ativos\n",
    "list_assets_url = \"https://api.nvcf.nvidia.com/v2/nvcf/assets\"\n",
    "\n",
    "# Substitua com o seu token JWT\n",
    "jwt_token = os.getenv(\"PHI3V_API_KEY\")\n",
    "\n",
    "# Cabeçalhos para autenticação\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Realizar a requisição GET para listar os ativos\n",
    "response = requests.get(list_assets_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Se a requisição for bem-sucedida, exiba os ativos\n",
    "    assets = response.json().get(\"assets\", [])\n",
    "    print(f\"Encontrados {len(assets)} ativos:\")\n",
    "    for asset in assets:\n",
    "\n",
    "        print(f\"ID: {asset.get('assetId')}, URL: {asset.get('uploadUrl')}, Descrição: {asset.get('description')}\")\n",
    "        print(asset.keys())\n",
    "else:\n",
    "    print(f\"Erro ao listar os ativos: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset ID: None\n",
      "URL: None\n",
      "Descrição: None\n",
      "Tipo de conteúdo: None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL da API para consultar o ativo específico\n",
    "asset_id = \"eb7f3024-9a5a-4876-ad65-52f3e23ca7e2\"\n",
    "get_asset_url = f\"https://api.nvcf.nvidia.com/v2/nvcf/assets/{asset_id}\"\n",
    "\n",
    "# Cabeçalhos para autenticação\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {jwt_token}\",\n",
    "    \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Realizar a requisição GET\n",
    "response = requests.get(get_asset_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    asset = response.json()\n",
    "    print(f\"Asset ID: {asset.get('assetId')}\")\n",
    "    print(f\"URL: {asset.get('uploadUrl')}\")\n",
    "    print(f\"Descrição: {asset.get('description')}\")\n",
    "    print(f\"Tipo de conteúdo: {asset.get('contentType')}\")\n",
    "else:\n",
    "    print(f\"Erro ao consultar o ativo: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asset': {'assetId': 'eb7f3024-9a5a-4876-ad65-52f3e23ca7e2',\n",
       "  'description': 'MQ_chat_image',\n",
       "  'contentType': 'image/png',\n",
       "  'createdAt': '2025-01-08T18:27:13Z'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodeys/.cache/pypoetry/virtualenvs/pandd-imagens-A4lvQ6DR-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor \n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3-vision-128k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_id, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# use _attn_implementation='eager' to disable flash attention\u001b[39;00m\n\u001b[1;32m     10\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|image_1|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWhat is shown in this image?\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHaving clear and pre-defined goals for meetings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKnowing where to find the information I need for a meeting\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnderstanding my exact role and responsibilities when I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm invited\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHaving tools to manage admin tasks like note-taking or summarization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHaving more focus time to sufficiently prepare for meetings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Each category has an associated bar indicating the level of agreement, measured on a scale from 0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m to 100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m     15\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide insightful questions to spark discussion.\u001b[39m\u001b[38;5;124m\"\u001b[39m} \n\u001b[1;32m     16\u001b[0m ] \n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pandd-imagens-A4lvQ6DR-py3.12/lib/python3.12/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pandd-imagens-A4lvQ6DR-py3.12/lib/python3.12/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import requests \n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor \n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n",
    "] \n",
    "\n",
    "url = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \n",
    "image = Image.open(requests.get(url, stream=True).raw) \n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n",
    "\n",
    "# remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO https://docs.api.nvidia.com/nim/reference/multimodal-apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandd-imagens-A4lvQ6DR-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
